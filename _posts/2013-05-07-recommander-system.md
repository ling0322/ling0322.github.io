--- 
layout: post
title: 使用矩阵分解(SVD)实现推荐系统
category: 机器学习
tags:
  - 推荐系统
  - 机器学习
intro: "使用矩阵奇异值分解(SVD)完成一个推荐系统的一些经验，知识以及实验数据(百度电影推荐算法大赛)。"
---

这个学期Web智能与社会计算的大作业就是完成一个推荐系统参加[百度电影推荐算法大赛](http://openresearch.baidu.com/activityindex.jhtml?channelId=300)，成绩按照评测数据给分。老师介绍了N种方法包括基于内容的、以及协同过滤等等，不过他强烈建议使用矩阵奇异值分解的办法来做。也正因为是这个原因，我们一共8组其中6组的模型都是SVD。

这个比赛就是提供给你用户对电影的评分、电影的TAG、用户的社会关系(好友)、用户的观看纪录信息。其中用户对电影的评分满分是5分，大约8k用户、1w电影，然后根据以上的信息预测用户对某些电影的评分。然后评测结果就是跟用户实际评分的RMSE值。

SVD就是一种主成分分析以及降维的方法，大体思路就是一个对称矩阵A可以分解成对角形

`\( A = P^{T}\Lambda P \)`

其中`\( \Lambda \)`为对角阵，它对角线上的元素就是矩阵A的特征值。特征值对应的特征向量是互相正交的，这个也就是对称矩阵的性质。在这些特征值中大的特征值以及它对应的特征向量对于整个结果(就是乘出来的A)来说比较重要，相比之下比较小的特征特征值对应的特征向量对于结果的影响就比较小，因此将这些小的特征值以及其对应的特征向量去掉，产生结果`\( {A}' \)`来说，`\( {A}' \)`仍旧是非常接近于A。这些被去掉的东西可以认为是A中的噪音。因此`\( {A}' \)`可以写作

`\( {A}' = U^{T}\Lambda V \)`

但是现实生活中很少有矩阵是对称的，比如上面的用户-电影矩阵。因此就诞生了奇异值分解的方法

`\( A = U^{T}\Delta V \)`

其中`\( \Delta \)`和`\( \Lambda \)`一样是一个对角阵，但是对角线上的元素就不是特征值了而是矩阵A的奇异值(至于为什么叫奇异值我也不知道，这个很奇异吗？)。A的奇异值就等于`\( AA^{T} \)`的特征值的根号。然后经过各种复杂的运算这个分解就完成了。奇异值分解的用途很广，主要是用作降维以及主成分分析。在自然语言中LSA主题模型就是基于奇异值分解，另外计算词与词之间的关系也要用到SVD。

使用SVD做推荐系统也可以看成是主成分分析的过程(此外还有另外一种解释就是将用户和电影投影到同一个向量空间中[1])，找到用户-电影矩阵中最大的r个奇异值，进行分解构成r行的矩阵U(代表用户)和r列的矩阵V(代表电影)，这个就是各个用户对电影打分的主要的影响因素，依据这个就可以对于用户的打分进行预测。

但是这里情况很特殊，用户-电影矩阵十分稀疏，大约只有1%左右的项是有值的，对于这些项全部用0来填这个明显就是表示用户对这些电影打0分，这个是不合逻辑的，用平均值或者随机值填充效果也肯定不好。因此目前使用它的一种近似形式，对于用户-电影矩阵`\( A_{m\times n} \)`，找到矩阵`\( Q_{r\times m} \)`、`\( T_{r\times n} \)`使得`\( Q^{T}T \)`尽量相似于A。这个可以转化成，对于每个评分`\( r_{ui} \)`找到 `\( q_{u}\in Q, t_{i}\in T \)` 使得

`\( \sum_{u, i} (r_{ui} - q_{i}^{T}p_{u}) + \lambda(\left \| q_{i} \right \|^{2} + \left \| p_{u} \right \|^{2} ) \)`

最小，其中后半部分是防止过拟合而加入的因子。这个很明显可以使用随机梯度下降算法进行求解(关于随机梯度下降算法可以参考斯坦福机器学习第二课)

`\( e_{ui} \overset{\underset{\mathrm{def}}{}}{=}r_{ui} - q_{i}^{T}p_{u} \)`

`\( q_{i} \leftarrow q_{i} + \gamma (e_{ui}\cdot p_{u} - \lambda\cdot q_{i}) \)`  
`\( p_{u} \leftarrow p_{u} + \gamma (e_{ui}\cdot q_{i} - \lambda\cdot p_{u}) \)`

其中`\( \gamma \)`就是迭代的步长，我们使用Go语言实现了以上的迭代过程，以下是一些实验的结果数据

0. `\( \gamma \)`＝0.001,  `\( \lambda \)`=0.01, iter=100, r=20, RMSE=0.6483
0. `\( \gamma \)`＝0.001,  `\( \lambda \)`=0.01, iter=1000, r=20, RMSE=0.6618
0. `\( \gamma \)`＝0.001,  `\( \lambda \)`=0.05, iter=1000, r=20, RMSE=0.6298
0. `\( \gamma \)`＝0.001,  `\( \lambda \)`=0.1, iter=1000, r=20, RMSE=0.6534
0. `\( \gamma \)`＝0.001,  `\( \lambda \)`=0.05, iter=1000, r=50, RMSE=0.6298

其中iter表示迭代次数、`\( \lambda \)`是防过拟合因子的系数、RMSE是在百度平台上跑的结果(越小越好)。其中比较1和2可以很明显的发现`\( \lambda \)`在取0.01的时候还是出现了过拟合现象，因此在`\( \lambda \)`=0.05的情况下是所有实验数据中最好的。r是因子的个数，这里对结果影响不大。

`\( \gamma \)`的取值也非常重要，因为在梯度下降中`\( \gamma \)`过小会收敛很慢，`\( \gamma \)`过大会错过最佳的点。其实最好的解决方法就是使得`\( \gamma \)`的值随着变化率动态改变，本来在代码中实现了这个功能的，但是因为实现之后发现程序太慢了，所以就去掉了。

最后在课程结束Presentation的时候，老师告诉我们其实好的推荐系统应该将TAG、用户社交关系等信息都加进去，这样的数据的结果才能够有大幅度的提升(因为我们都是用SVD做，结果大同小异)。此外，同学将均值以及用户的偏移加进去之后最好的结果是0.625左右(详细情况可以看[推荐系统相关算法(1)：SVD](http://www.cnblogs.com/FengYan/archive/2012/05/06/2480664.html)这篇文章)。

另外推荐一篇论文[Matrix Factorization Techniques for Recommender Systems][1]


[1]: http://www2.research.att.com/~volinsky/papers/ieeecomputer.pdf‎

