<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>al3xandr3</title>
 <link type="application/atom+xml" rel="self" href="http://domain/atom.xml"/>
 <link type="text/html" rel="alternate" href="http://domain/"/>
 <updated>2013-05-07T22:30:58+08:00</updated>
 <id>http://al3xandr3.github.com/</id>
 <author>
   <name>al3xandr3</name>
   <email>al3xandr3@gmail.com</email>
 </author>

 
 <entry>
   <title>使用矩阵分解做推荐系统</title>
   <link href="http://al3xandr3.github.com/2013/05/07/recommander-system.html"/>
   <updated>2013-05-07T00:00:00+08:00</updated>
   <id>http://al3xandr3.github.com/2013/05/07/recommander-system</id>
   <category term="机器学习" label="机器学习" />
   
   <content type="html">&lt;p&gt;&lt;code&gt;\(A^{2}_{2} \)&lt;/code&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>关于分词以及常用软件的简单介绍</title>
   <link href="http://al3xandr3.github.com/2013/02/13/cws-intro.html"/>
   <updated>2013-02-13T00:00:00+08:00</updated>
   <id>http://al3xandr3.github.com/2013/02/13/cws-intro</id>
   <category term="自然语言" label="自然语言" />
   
   <content type="html">&lt;p&gt;因为自然语言处理大作业的关系，最近看一些关于分词算法和软件的介绍。现在已经看的差不多了，因为网络上关于分词的资料比较少，大多数还停留在字典分词阶段，所以为了方便需要了解着一个方面的人，总结一下，写成这篇日志。&lt;/p&gt;

&lt;p&gt;首先说一下目前主流的分词算法，在 &lt;a href=&quot;http://zhangkaixu.github.com/bibpage/cws.html&quot;&gt;http://zhangkaixu.github.com/bibpage/cws.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;这里zhangkaixu大大维护了一些比较著名的分词算法论文，如果想去搞研究的话可以去这里坑几篇论文学习学习。&lt;/p&gt;

&lt;h2&gt;主流分词软件&lt;/h2&gt;

&lt;p&gt;挑重要的讲，目前的分词软件很多，也比较杂。这里列举一些效果比较好的分词软件。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Stanford Word Segmenter （Java，GPL） 老牌美帝斯坦佛大学NLP小组出品&lt;/li&gt;
&lt;li&gt;nlpbamboo (C++, BSD） PostgreSQL的大大们制作的一个基于CRF++库的分词软件&lt;/li&gt;
&lt;li&gt;ICTCLAS （闭源但是提供非商业使用lib）中科院出品也是目前比较广泛的分词软件&lt;/li&gt;
&lt;li&gt;LTP （主要C++, 很神奇的LTP开源协议） 哈工大的一个自然语言处理平台，已经做到浅层语义分析了，分词是其中的一个模块。&lt;/li&gt;
&lt;li&gt;fudannlp （Java，LGPL） 复旦大学的语言处理平台，与LTP类似，已经做到依存句法分析以及指代消歧了。用Java编写LGPL协议，非常推荐。&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;分类&lt;/h2&gt;

&lt;p&gt;分词算法在20世纪80年代就有研究，不过基于当时的技术条件所限，大多数就是原始的机械分词算法。比如，最大匹配算法，mmseg等。&lt;/p&gt;

&lt;p&gt;关于原始的机械分词算法 &lt;a href=&quot;http://www.cnblogs.com/alic/articles/1215001.html&quot;&gt;http://www.cnblogs.com/alic/articles/1215001.html&lt;/a&gt; 这篇blog有很详细的介绍。&lt;/p&gt;

&lt;p&gt;之后随着统计算法在自然语言处理领域地位的奠定以及机器学习的兴起，基于统计和机器学习的分词算法逐渐成为主流。根据机器学习方法的分类，分词算法也可以分成无监督分词，半监督分词以及有监督的分词。目前有监督的分词以及半监督的分词已经研究的比较成熟。对于这种分词算法，大致可以分为四类。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;第一类是&lt;strong&gt;生成式的基于词&lt;/strong&gt;的分词方法，这类具有代表的是中科院的ICTCLAS等等比较经典的分词软件。&lt;/li&gt;
&lt;li&gt;第二类是&lt;strong&gt;判别式的基于词&lt;/strong&gt;的分词方法，这类中的算法比较少，其中一个是基于平衡感知机的分词。&lt;/li&gt;
&lt;li&gt;第三类是&lt;strong&gt;生成式基于字&lt;/strong&gt;的生成式分词方法，比如Wang(2009)所提出的n元模型分词算法。&lt;/li&gt;
&lt;li&gt;第四类也是目前主流的是&lt;strong&gt;基于字的判别式&lt;/strong&gt;分词方法，主要是最大熵模型和条件随机场模型。几乎全部2003年之后提出的分词算法都与这一类别有关，比如哈工大的LTP和Stanford Word Segmenter就是采用CRF模型。&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;此外，分词算法还可以根据在自然预言处理中的步骤来分，比如纯分词，就是除了分词之外什么都不做，大多数基于字的分词算法都属于这一类。此外将分词和词性标注结合在一起完成，比如ICTCLAS。还有基于语义网络分词的算法。总的来说，结合额外信息越多，对于分词结果就越好。比如ICTCLAS将分词与词性标注结果结合起来，效果就比光光使用一元概率模型（ICTCLAS）效果要好。&lt;/p&gt;

&lt;h2&gt;分词模型&lt;/h2&gt;

&lt;p&gt;分词模型大致可以分为判别式和生成式，以及基于词和基于字的方法。主要使用到的模型为: n-Gram，Percepton，HMM，SVM，ME，CRF。&lt;/p&gt;

&lt;p&gt;判别式和生成式的区别就是判别式是计算P(Y|X)的条件概率，但是生成式则是计算P(X1, X2)的联合概率，通常来说判别式的效果要比生成式的要好，这一分类比较复杂，不好解释，涉及到很多数学模型，详情可以去看看相关资料。&lt;/p&gt;

&lt;p&gt;基于词和基于字的分词的区别就是基于词的算法将词看成算法中最小单元，比如这句话&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;结合/成/分子/时&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;在基于词的分词算法中，主要用到一个词典，“结合”、“成”、“分子”在词典中，它们是分词中的最小单元，不可拆分，一旦拆分就会出现分词错误。但是基于字的分词算法则不同，它将每个字看成是一个单元，通常和序列标注相结合，对每个字进行标注，然后得出分词结果。比如&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;结合成分子时&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;使用CRF等序列标注模型得出的序列标注结果是&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;B E S B E S&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;其中B表示一个词的开头，E表示一个词的结尾，S表示单个字作为词。然后分词的结果经过一些小处理就可以得出了。&lt;/p&gt;

&lt;p&gt;通常来说基于词的分词算法在词典比较全，文章比较正式的时候效果比较好。但是总是会遇见一些比较变态的词典中没有的词，其中最有代表的就是人名地名还有商标名等等。所以一般这些基于词分词软件还必须要加上识别人名地名的功能。&lt;/p&gt;

&lt;p&gt;基于字的分词算法主要是依赖于某些字在构词方面的特征，比如“的”字通常就是单个字出现，有很大的几率是标注为S，“化”字比较喜欢出现在词的末尾，比如现代化，工业化等等。根据这些信息进行标注分词，它对于词典中不存在的词效果识别几率比较大，但是对于词典中的词可能会识别错误，另外还会出现千奇百怪的分词错误，比如“沙把”等等。
因此，现在多数的分词软件在基于字的基于上，或多或少的结合了一点基于词的特征。&lt;/p&gt;

&lt;h2&gt;相关资源&lt;/h2&gt;

&lt;p&gt;关于中文分词的课程，可以去搜索一下刘群教授的课程录像或者课件，非常好。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CRF模型常用的一个库CRF++ &lt;a href=&quot;http://crfpp.googlecode.com/svn/trunk/doc/index.html&quot;&gt;http://crfpp.googlecode.com/svn/trunk/doc/index.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;一篇关于CRF++分词的blog &lt;a href=&quot;http://yongsun.me/2008/03/%E5%AE%9E%E9%AA%8Ccrf/&quot;&gt;http://yongsun.me/2008/03/%E5%AE%9E%E9%AA%8Ccrf/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bakeoff-2005官网这里有4个可以免费测试的语料庫 &lt;a href=&quot;http://www.sighan.org/bakeoff2005/&quot;&gt;http://www.sighan.org/bakeoff2005/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;人民日报1998年1月标注语料庫是可以免费使用的&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;结束以及一些思考&lt;/h2&gt;

&lt;p&gt;大致走马观花的讲了一下中文分词领域的一些模型以及目前的一些分词软件。在最近特别是Web的兴起，伴随着大量新词比如“基友”、“妹抖”的出现，对于分词也是一个挑战。目前新词识别率比较高的CRF模型在新词识别方面最高也只达到70%左右的正确率。且大多数的中文分词语料均是基于正规文本比如《人民日报》，所以使用在Web内容中有一定的局限。&lt;/p&gt;

&lt;p&gt;期待有更好的分词模型被提出吧=w=&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>使用韩梅梅(HMM)模型(又称隐马模型)解决拼音输入的纠错问题</title>
   <link href="http://al3xandr3.github.com/2012/08/26/hmm-pinyin.html"/>
   <updated>2012-08-26T00:00:00+08:00</updated>
   <id>http://al3xandr3.github.com/2012/08/26/hmm-pinyin</id>
   <category term="机器学习" label="机器学习" />
   
   <content type="html">&lt;p&gt;拼音纠错就是将用户输入的汉语拼音进行纠错，这个貌似说跟没说一个样。打个比方吧，用户输入zhege(这个)的时候，不小心输成了zhrge。这个时候就需要拼音纠错将zhr ge自动识别成zhe ge，因为完整的拼音zh后面不可能跟着r。&lt;/p&gt;

&lt;p&gt;最近在看关于HMM模型的书，突然想到拼音纠错问题可以使用HMM来解决。道理很简单，将正确的拼音，比如zhege，作为内部状态序列；将用户的输入，比如zhrge，作为观测序列，建立HMM后使用viterbi算法从观测序列取猜测最有可能的内部序列就实现了纠错的过程。这样就是一个很典型的隐马模型了，每个内部状态都有一定的几率产生正确的或者错误的观测结果，比如内部状态e，有一定几率产生正确的观测结果e，也有可能产生错误的观测结果r，这个就是发射矩阵(Emit Matrix)；在拼音中h后面有一定的概率是e，但是后面是r的概率就接近于0了，这个就是HMM里面的转移矩阵(Trans Matrix)。&lt;/p&gt;

&lt;p&gt;为了简单的实现用HMM模型实现拼音的纠错，进行三个假设&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;所有输入的拼音均为完成的拼音，比如zhege，但是zheg或者zge就不是完整的拼音&lt;/li&gt;
&lt;li&gt;所有可能出错的按键均为某输入的按键的旁边两个键，比如e只可能错误的输入成r或者w，h只可能错成g或者j&lt;/li&gt;
&lt;li&gt;任何汉字的拼音的出现都是独立的，就比如zhe和ge是独立的(虽然现实中是有关系的，zhe后面出现ge的概率很大)，相互的出现对彼此都没有影响&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;有了以上的两个假设，问题解决起来就方便多了。转移矩阵可以从Google输入法的词库文件中训练出来，而发射矩阵需要自己定义，将每个按键本身和旁边两个键的概率自己设定一下就可以了，比如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;'b': {'b': prob_t_c, 'v': prob_t_i, 'n': prob_t_i},
'c': {'c': prob_t_c, 'x': prob_t_i, 'v': prob_t_i},
'd': {'d': prob_t_c, 's': prob_t_i, 'f': prob_t_i},
'e': {'e': prob_t_c, 'w': prob_t_i, 'r': prob_t_i},
....
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里的prob_t_c就是正确的概率，prob_t_i就是错误的概率。如果取prob_t_c = 0.8, prob_t_i = (1 - prob_t_c) / 2 = 0.1的话，那么b就有0.8的概率被正确输入，而有0.1的概率分别被错误的输入成v和n。关于prob_t_c到底取多少才是最优的这个问题没有取深究过，不过使用了一组数据小观测了一下在30%错误率情况下取prob_t_c = 0.8效果最佳。&lt;/p&gt;

&lt;p&gt;此外，为了解决拼音的切分问题，将可能作为拼音结尾的几个字母分成两种状态。即出现在拼音首部以及中部的状态x，和只出现在拼音尾部的状态x'。其中状态x'的转移概率向量为起始状态('^')的转移概率向量，因为这里假设汉字的拼音是独立的，zhe这个拼音结束以后，任何的字母的出现的概率与最开始某字母在拼音首部出现的概率是一样的；x'的发射概率向量与x的发射概率向量是相同的。这样就可以解决拼音的切分问题了。比如shen，从e状态转移到n'状态就表示拼音结束，而从e状态转移到n状态则表示拼音还没有结束。&lt;/p&gt;

&lt;p&gt;将HMM模型解决好后，就可以使用viterbi算法根据观测序列取猜测最有可能的内部序列来完成纠错了。&lt;/p&gt;

&lt;p&gt;以下是纠错的结果(prob_t_c = 0.8的情况下)：&lt;/p&gt;

&lt;p&gt;30%单个字母错误率的情况下，1-viterbi算法召回率约为68%，3-viterbi算法召回率约为89%，虽然数据看上去让人很伤心，但是实际上测试数据中包括了大量比如wenguachisnvo这种错的离谱的拼音，就算是人也不能很容易的猜出正确的结果(wen'hua'chuan'bo')。总体上来说还是不错的。&lt;/p&gt;

&lt;p&gt;10%单个字母错误率情况下3-viterbi算法召回率则达到了95%.&lt;/p&gt;

&lt;p&gt;如果想再提高正确率的话，简单的办法由两种。第一是使用n&gt;3的n-viterbi算法，第二则是使用阶数更高的HMM模型。不过任何一种带来的时间/空间复杂度的提升都是指数级别的。&lt;/p&gt;

&lt;p&gt;相关代码放在了https://github.com/ling0322/hmm-pinyin里面，其中result.txt为10%单个ziu错误概率情况下的运行结果，result2.txt为30%下的运行结果。&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>使Python 2.x字符串编码的小结</title>
   <link href="http://al3xandr3.github.com/2012/06/26/python-encoding.html"/>
   <updated>2012-06-26T00:00:00+08:00</updated>
   <id>http://al3xandr3.github.com/2012/06/26/python-encoding</id>
   <category term="代码" label="代码" />
   
   <content type="html">&lt;p&gt;如果你 经常遇到这种错误提示的信息: UnicodeEncodeError: ‘ascii’ codec can’t encode characters in position 0-1: ordinal not in range(128), 或者杯具的发现明明在Eclipse中写的程序能够正常运行然后到了终端下面就跳出以上的一段话. 那么, 就证明你和我一样, 遇到了悲催的Python的编码问题了.&lt;/p&gt;

&lt;p&gt;之前在用Python语言写我的毕业设计, 然后各种没有问题, 直到整个东西完成了, 突发奇想想去试一下对中文的支持. 然后你懂的, 就弹出了以上一串恶心的错误提示, 然后改半天, 各种改, 各种错误, 然后各种想砸键盘. 其实之前的一篇日志中也说到了, 解决这一类的问题最好的方法就是在程序开头加上以下几行代码:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import sys
reload(sys)
sys.setdefaultencoding(“utf-8″)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;那么就可助你解决几乎95%的这种问题, 但是如果想刨根问底的话, 就需要去了解很多东西了.&lt;/p&gt;

&lt;p&gt;首先, 这个就是Python语言本身的问题. 因为在Python 2.x的语法中, 默认的str并不是真正意义上我们理解的字符串, 而是一个byte数组, 或者可以理解成一个纯ascii码字符组成的字符串, 与Python 3中的bytes类型的变量对应; 而真正意义上通用的字符串则是unicode类型的变量, 它则与Python 3中的str变量对应. 本来应该用作byte数组的类型, 却被用来做字符串用, 这种看似奇葩的设定是Python 2一直被人诟病的东西, 不过也没有办法, 为了与之前的程序保持兼容.&lt;/p&gt;

&lt;p&gt;在Python 2中作为两种字符串类型, str与unicode之间就需要各种转换的方式. 首先是一种显式转换的方式, 就是encode和decode两种方法. 在这里这两货的意思很容易被搞反, 科学的调用方式是:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;str --- decode方法 ---&gt; unicode&lt;br/&gt;
unicode --- encode方法 ---&gt; str&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;比如:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; type('x')
&amp;lt;type 'str'&amp;gt;
&amp;gt;&amp;gt;&amp;gt; type('x'.decode('utf-8'))
&amp;lt;type 'unicode'&amp;gt;
&amp;gt;&amp;gt;&amp;gt; type(u'x'.encode('utf-8'))
&amp;lt;type 'str'&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个逻辑是这样的, 对于unicode字符串使用utf-8编码进行编码, 即调用encode('utf-8')方法生成byte数组类型的结果. 相反对于byte数组进行解码, 生成unicode字符串. 这个新手表示理解不能, 不过熟悉了就见怪不怪了.&lt;/p&gt;

&lt;p&gt;另外是隐式的转换, 和C语言中的int转double类似, 当一个unicode字符串和一个str字符串进行连接的时候会默认自动将str字符串转换成unicode类型然后再连接. 而这个时候使用的编码方式则是系统所默认的编码方式. 使用:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import sys
print sys.getdefaultencoding()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以得到当前默认的编码方式, 是不是'ascii'? 是的话就恭喜你中彩了~!! 在这个时候如果有以下一行代码就保证会出错:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; x = u'喵'
&amp;gt;&amp;gt;&amp;gt; x
u'\u55b5'
&amp;gt;&amp;gt;&amp;gt; y = x.encode('utf-8')
&amp;gt;&amp;gt;&amp;gt; x + y
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;&lt;p&gt;Traceback (most recent call last):&lt;br/&gt;
File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;&lt;br/&gt;
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe5 in position 0: ordinal not in range(128)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;x是unicode类型的变量, y是x经过encode后的结果是str类型的变量. x + y的时候, 首先要将y转换成unicode字符串, 那么使用什么编码格式转换呢, 用utf-8还是gb2312或者还是utf-16? 这个时候就要根据sys.getdefaultencoding()来确定, 而sys.getdefaultencoding()是'ascii'编码, 在ascii字符表中不存在0xe5这种大于128的字符存在, 所以当然报错啦! 通过加入&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import sys
reload(sys)
sys.setdefaultencoding(“utf-8″)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;则可以将默认的编码转换格式变成utf-8, 且大多数情况下, 程序中的字符串是通过utf-8来编码的, 所以只要加上以上三行就可以了.&lt;/p&gt;

&lt;p&gt;但是有没有觉得, 加上这些会使得代码有些dirty? 咳, 至少对于我来说确实很dirty. 所以我觉得平时写程序的过程中要养成尽量使用显示的转换的习惯, 并且要明确某个函数返回的到底是str还是unicode, 凡是str的主动decode成unicode, 不要将两者混淆掉, 这样写出来的代码才比较干净. 此外还可以在代码最上方加入'from &lt;strong&gt;future&lt;/strong&gt; import unicode_literals'可以默认将用户自定义字符串变成unicode类型.&lt;/p&gt;

&lt;p&gt;最后我想大吼一声Python 2.x中str不是字符串, 而是B♂Y♂T♂E♂数♂组~!!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Linux下Simsun字体英文关AA的设置</title>
   <link href="http://al3xandr3.github.com/2012/02/11/linux-font-aa.html"/>
   <updated>2012-02-11T00:00:00+08:00</updated>
   <id>http://al3xandr3.github.com/2012/02/11/linux-font-aa</id>
   <category term="其他" label="其他" />
   
   <content type="html">&lt;p&gt;在~目录下建立一个.fonts.conf文件里面的内容为&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;match target=&quot;font&quot;&amp;gt;
&amp;lt;test name=&quot;pixelsize&quot; compare=&quot;less_eq&quot;&amp;gt;&amp;lt;double&amp;gt;16.5&amp;lt;/double&amp;gt;&amp;lt;/test&amp;gt;
&amp;lt;test name=&quot;pixelsize&quot; compare=&quot;more_eq&quot;&amp;gt;&amp;lt;double&amp;gt;10.5&amp;lt;/double&amp;gt;&amp;lt;/test&amp;gt;
&amp;lt;test qual=&quot;any&quot; name=&quot;family&quot;&amp;gt;&amp;lt;string&amp;gt;SimSun&amp;lt;/string&amp;gt;&amp;lt;/test&amp;gt;
&amp;lt;edit name=&quot;antialias&quot; mode=&quot;assign&quot;&amp;gt;&amp;lt;bool&amp;gt;false&amp;lt;/bool&amp;gt;&amp;lt;/edit&amp;gt;
&amp;lt;/match&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是将[11px, 16px]之间的宋体强制点阵，解决宋体的网页英文看上去各种不爽的问题&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>MapReduce笔记</title>
   <link href="http://al3xandr3.github.com/2012/02/10/mapreduce-notes.html"/>
   <updated>2012-02-10T00:00:00+08:00</updated>
   <id>http://al3xandr3.github.com/2012/02/10/mapreduce-notes</id>
   <category term="云计算" label="云计算" />
   
   <content type="html">&lt;p&gt;Google的MapReduce是一种分布式计算的框架，它的设计的目标就是在廉价PC组成的集群中，并行处理大规模的数据。另外的一个设计思路就是限制计算的模型以在分布以及运算上面得以优化。&lt;/p&gt;

&lt;p&gt;整个过程分为两部分，Map和Reduce这两个和函数式编程语言都有很大的联系。Map即是将数据从一个集合映射到另外一个以(key, value)键值对元组组成的集合，而Reduce即为在(key, value)键值对的集合中合并key相同的元素，最终生成结果。这样说很复杂，举论文中的一个小例子来说明一下：&lt;/p&gt;

&lt;p&gt;Count of URL Access Frequency问题，即从一个web access的log里面去计算每个url被访问的次数。那么Map的工作就是根据每一条记录生成(URL, 1)的一个键值对集合，Reduce就将相同的key值(URL)的元素合并起来，将它们的value（每一项均为1）加起来，得出最终的集合就是要求的结果。&lt;/p&gt;

&lt;p&gt;由于Reduce是从Map的机器中pull得到Map的结果, 对于普通的磁盘来说, 多个Reduce进程同时通过网络读取Map的结果的时候会严重拖累读取的速度.&lt;/p&gt;

&lt;p&gt;一般Map/Reduce的实例数是集群中worker数量的几倍, 原因是当一个worker挂掉的时候, 可以将它的Map/Reduce工作平均分给集群中的其他worker, 以免拖累整体完成速度.&lt;/p&gt;

&lt;p&gt;针对木桶效应的问题, 每当Master发现一个操作要接近结束的时候就开始一群备用的worker和最开始的worker去同时执行剩下的操作. 只要备用的和最开始的worker其中之一完成了整个过程就完成了. 这样主要是为了解决某些worker因为硬件问题或者负载问题而导致的运行速度低下. 应用了这个机制之后某些计算速度提升高达44%&lt;/p&gt;

&lt;p&gt;MapReduce处理机器之间性能不对等的基本思路就是将整个计算的过程分成接很小规模的计算（微粒），然后动态的将这些微粒分配给各个worker，快的worker就处理更多的数据，以此来调节整个运算的平衡性.&lt;/p&gt;

&lt;p&gt;论文中总结的MapReduce的优点是：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;编程简单=&gt;即使没有学过并行编程的人来说在这个框架下写程序也是非常容易的事情，因为它隐藏了许多并行编程的内部问题比如机器当掉的处理，本地优化和负载均衡。&lt;/li&gt;
&lt;li&gt;很多计算都是可以用Map和Reduce两个过程进行解决&lt;/li&gt;
&lt;li&gt;MapReduce能够运行在数千台机器规模的集群上，运用计算机数量的优势处理大规模的数&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>Hello World</title>
   <link href="http://al3xandr3.github.com/2012/01/13/hello-world.html"/>
   <updated>2012-01-13T00:00:00+08:00</updated>
   <id>http://al3xandr3.github.com/2012/01/13/hello-world</id>
   <category term="其他" label="其他" />
   
   <content type="html">&lt;p&gt;终于花了三四天时间这货暂时搞定了，虽然是还想去加一个评论功能，但是根据以往经验，评论功能有和没有一个样，于是果断就这样了。咳，其实本意是要写一个可以同步Twitter/Tumblr的东西的，现在也深知工程量巨大，就暂时搁一下了，最后才发现自己是完全在造wordpress的轮子。&lt;/p&gt;

&lt;p&gt;虽然这样，但是这货用python+sqlite很明显比用php+mysql的wordpress占用资源少（128MB的VPS表示压力很大。&lt;/p&gt;

&lt;p&gt;这货的UI是在wordpress的belle主题上改的，依照作者的要求footer的地方原有的东西不动。还是我最喜欢的一个wordpress主题了（咳 就这样吧，恩，这个粗糙货终于完成了，恩，恩
P.S. 源代码可以在这里下载到ww&lt;/p&gt;
</content>
 </entry>
 

</feed>